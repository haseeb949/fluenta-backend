# -*- coding: utf-8 -*-
"""FlaskAPI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uihvsfycjhZr7FmzC4x3yC8hZX0ChuKV
"""

import os
import numpy as np
import librosa
!pip install noisereduce==2.0.1
import noisereduce as nr
import joblib
from flask import Flask, request, jsonify
from datetime import datetime
!pip install flask pyngrok librosa noisereduce scikit-learn joblib
!python app.py
!apt install ffmpeg
!pip install pydub
from pydub import AudioSegment

from google.colab import files
uploaded = files.upload()

# Load model
MODEL_PATH = "/content/drive/MyDrive/Fluenta_Models/stutter_model_final.pkl"
SCALER_PATH = "/content/drive/MyDrive/Fluenta_Models/scaler_final.pkl"
FEATURE_COUNT_PATH = "/content/drive/MyDrive/Fluenta_Models/feature_count.pkl"

model = joblib.load(MODEL_PATH)
scaler = joblib.load(SCALER_PATH)
feature_count = joblib.load(FEATURE_COUNT_PATH)

# Flask App
app = Flask(__name__)
predictions_log = []
def preprocess_audio(audio, sr):
    audio, _ = librosa.effects.trim(audio, top_db=20)
    audio = librosa.util.normalize(audio)
    try:
        audio = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.8)
    except:
        pass
    if sr != 16000:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
        sr = 16000
    min_length = int(0.5 * sr)
    if len(audio) < min_length:
        audio = np.pad(audio, (0, min_length - len(audio)), mode='constant')
    return audio, sr

# Feature Extraction
def extract_features(file_path):
    try:
        audio, sr = librosa.load(file_path, sr=None)
        audio, sr = preprocess_audio(audio, sr)

        features = []
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
        features.extend(np.mean(mfccs.T, axis=0))
        features.extend(np.std(mfccs.T, axis=0))

        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
        rms = librosa.feature.rms(y=audio)[0]

        features += [np.mean(spectral_centroids), np.std(spectral_centroids),
                     np.mean(spectral_rolloff), np.std(spectral_rolloff),
                     np.mean(zcr), np.std(zcr),
                     *np.mean(chroma.T, axis=0),
                     np.mean(rms), np.std(rms)]

        return np.array(features)
    except Exception as e:
        print(f"Feature extraction error: {e}")
        return None

# Prediction Endpoint
@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({'error': 'No audio file uploaded'}), 400

    file = request.files['file']
    filename = file.filename
    filepath = f"/tmp/{filename}"
    file.save(filepath)

    #Convert any non-WAV file to WAV automatically
    if not filename.lower().endswith(".wav"):
        wav_path = f"/tmp/converted.wav"
        try:
            sound = AudioSegment.from_file(filepath)
            sound.export(wav_path, format="wav")
            os.remove(filepath)
            filepath = wav_path
            print(f"Converted {filename} to WAV format.")
        except Exception as e:
            return jsonify({"error": f"File conversion failed: {e}"}), 500

    # Extract features
    features = extract_features(filepath)
    os.remove(filepath)

    if features is None:
        return jsonify({'error': 'Feature extraction failed'}), 500

    if len(features) != feature_count:
        return jsonify({'error': f'Feature count mismatch. Expected {feature_count}, got {len(features)}'}), 500

    features_scaled = scaler.transform([features])
    prediction = model.predict(features_scaled)[0]
    proba = model.predict_proba(features_scaled)[0] if hasattr(model, 'predict_proba') else None

    result_label = "Stutter" if prediction == 1 else "Non Stutter"
    confidence = round(float(proba[prediction]) * 100, 2) if proba is not None else None
    fluency_score = round(100 - confidence, 2) if result_label == "Stutter" else confidence

    feedback = (
        "Severe stuttering detected. Fluency score: 0%. Try to relax and slow down."
        if fluency_score <= 25 else
        "Moderate stuttering detected. Keep practicing!"
        if fluency_score <= 75 else
        "Great job! No stutter detected. Fluency score: 100%."
    )

    result = {
        "prediction": result_label,
        "confidence": confidence,
        "fluency_score": fluency_score,
        "feedback": feedback
    }

    # Track prediction
    predictions_log.append({
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "filename": filename,
        "prediction": result_label,
        "confidence": confidence
    })

    return jsonify(result)

# Stats Endpoint
@app.route('/stats')
def stats():
    if not predictions_log:
        return jsonify({"message": "No predictions yet"})

    total = len(predictions_log)
    stutter_count = sum(1 for p in predictions_log if p["prediction"] == "Stutter")

    return jsonify({
        "total_predictions": total,
        "stutter_predictions": stutter_count,
        "fluent_predictions": total - stutter_count,
        "stutter_percentage": round(stutter_count / total * 100, 2)
    })

!ngrok config add-authtoken 33HVMsJ3w9V3ztrSLmbVKi0ZdMS_5orXJd2WC1qHP8A21YMwH

# Run with ngrok
from pyngrok import ngrok

@app.route('/')
def home():
    return jsonify({"message": "Enhanced Stuttering Detection API is running!"})

if __name__ == '__main__':
    public_url = ngrok.connect(5000)
    print(f"\nðŸš€ Public URL: {public_url}\n")
    app.run(port=5000)

!killall ngrok
!pkill -f ngrok
ngrok.kill()
!rm -rf /root/.ngrok2/ngrok.yml