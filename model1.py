# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q9IXU6bH7Dj20JHiBnyWe_rB57X7VGdX
"""

import pandas as pd
import numpy as np
import os
import librosa
import soundfile as sf
!pip install noisereduce==2.0.1
import noisereduce as nr
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import RobustScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# PREPROCESSING
def preprocess_audio(audio, sr):
    # 1. Remove silence
    audio, _ = librosa.effects.trim(audio, top_db=20)

    # 2. Normalize amplitude
    audio = librosa.util.normalize(audio)

    # 3. Noise reduction
    try:
        audio = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.8)
    except:
        pass

    # 4. Resample to 16kHz
    if sr != 16000:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
        sr = 16000

    # 5. Ensure minimum length
    min_length = int(0.5 * sr)
    if len(audio) < min_length:
        audio = np.pad(audio, (0, min_length - len(audio)), mode='constant')

    return audio, sr

def extract_features(file_path):
    try:
        audio, sr = librosa.load(file_path, sr=None)
        audio, sr = preprocess_audio(audio, sr)

        features = []

        # MFCCs (20 coefficients)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
        features.extend(np.mean(mfccs.T, axis=0))
        features.extend(np.std(mfccs.T, axis=0))

        # Spectral features
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        features.append(np.mean(spectral_centroids))
        features.append(np.std(spectral_centroids))

        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        features.append(np.mean(spectral_rolloff))
        features.append(np.std(spectral_rolloff))

        # Zero-crossing rate
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        features.append(np.mean(zcr))
        features.append(np.std(zcr))

        # Chroma features
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
        features.extend(np.mean(chroma.T, axis=0))

        # RMS Energy
        rms = librosa.feature.rms(y=audio)[0]
        features.append(np.mean(rms))
        features.append(np.std(rms))

        return np.array(features)

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

#  CUSTOM VOICE INTEGRATION
def load_custom_recordings(custom_path, verbose=True):
    features_list = []
    labels_list = []
    file_details = []

    if not os.path.exists(custom_path):
        print(f"Custom recordings path not found: {custom_path}")
        return features_list, labels_list, file_details

    # Define keywords for each class
    STUTTER_KEYWORDS = ['stutter', 'stuttering', 'disfluent', 'dysfluent', 'blocked', 'repetition']
    FLUENT_KEYWORDS = ['fluent', 'normal', 'nonstutter', 'non-stutter', 'clear']

    print(f"\n{'='*60}")
    print(f"Loading custom recordings from: {custom_path}")
    print(f"{'='*60}")

    files = [f for f in os.listdir(custom_path) if f.endswith('.wav')]

    if len(files) == 0:
        print(" No .wav files found in custom recordings folder!")
        return features_list, labels_list, file_details

    for file_name in sorted(files):
        file_path = os.path.join(custom_path, file_name)
        file_lower = file_name.lower()

        # Determine label based on filename
        label = None
        keyword_found = None

        # Check for stutter keywords
        for keyword in STUTTER_KEYWORDS:
            if keyword in file_lower:
                label = 1
                keyword_found = keyword
                break

        # Check for fluent keywords if not already labeled
        if label is None:
            for keyword in FLUENT_KEYWORDS:
                if keyword in file_lower:
                    label = 0
                    keyword_found = keyword
                    break

        # If still no label, warn and skip
        if label is None:
            print(f"SKIPPED: {file_name} (no recognizable label keyword)")
            print(f"   → Use keywords: {STUTTER_KEYWORDS + FLUENT_KEYWORDS}")
            continue

        # Extract features
        features = extract_features(file_path)

        if features is not None:
            features_list.append(features)
            labels_list.append(label)
            file_details.append({
                'filename': file_name,
                'label': 'Stutter' if label == 1 else 'Non-Stutter',
                'keyword': keyword_found
            })

            if verbose:
                label_str = "✓ Stutter" if label == 1 else "✓ Non-Stutter"
                print(f"{label_str:15} | {file_name:40} | keyword: '{keyword_found}'")
        else:
            print(f" FAILED: {file_name} (feature extraction error)")

    print(f"\n{'='*60}")
    print(f"Loaded {len(features_list)} custom recordings")
    print(f"   - Stutter: {sum(labels_list)}")
    print(f"   - Non-Stutter: {len(labels_list) - sum(labels_list)}")
    print(f"{'='*60}\n")

    return features_list, labels_list, file_details

# MAIN TRAINING PIPELINE


print("\n" + "="*60)
print("STUTTERING DETECTION MODEL TRAINING")
print("="*60)

# STEP 1: Load original dataset labels
print("\n[1/8] Loading original dataset labels...")
labels_df = pd.read_csv("/content/drive/MyDrive/fyp dataset/clips/labels.csv")
labels_df["label"] = labels_df[["Block", "Prolongation", "SoundRep", "WordRep", "Interjection"]].sum(axis=1)
labels_df["label"] = np.where(labels_df["label"] > 0, 1, 0)
labels_df["filepath"] = labels_df["filepath"].apply(lambda x: os.path.basename(x))

print(f"Loaded {len(labels_df)} label entries")
print(f"   Original dataset distribution:\n{labels_df['label'].value_counts()}")

# STEP 2: Extract features from original dataset
print("\n[2/8] Extracting features from original dataset...")
dataset_path = "/content/drive/MyDrive/fyp dataset/clips/clips"
features_list = []
labels_list = []
dataset_source = []  # Track which dataset each sample comes from

for idx, file_name in enumerate(os.listdir(dataset_path)):
    if file_name.endswith(".wav"):
        if idx % 100 == 0:
            print(f"   Processing file {idx}...")

        file_path = os.path.join(dataset_path, file_name)
        label_row = labels_df[labels_df["filepath"] == file_name]

        if label_row.empty:
            continue

        features = extract_features(file_path)
        if features is not None:
            features_list.append(features)
            labels_list.append(int(label_row["label"].values[0]))
            dataset_source.append("original")

print(f"Extracted {len(features_list)} samples from original dataset")

# STEP 3: Load and integrate custom recordings
print("\n[3/8] Loading custom voice recordings...")
custom_path = "/content/drive/MyDrive/myrecordings"
custom_features, custom_labels, custom_details = load_custom_recordings(custom_path, verbose=True)

# Add custom recordings to main dataset
if len(custom_features) > 0:
    features_list.extend(custom_features)
    labels_list.extend(custom_labels)
    dataset_source.extend(["custom"] * len(custom_features))

    print(f"Added {len(custom_features)} custom recordings to training set")
else:
    print(" No custom recordings loaded!")

# STEP 4: Create DataFrame and analyze
print("\n[4/8] Creating training dataset...")
df = pd.DataFrame(features_list)
df["label"] = labels_list
df["source"] = dataset_source

print(f"\n Dataset Statistics:")
print(f"   Total samples: {len(df)}")
print(f"\n   By source:")
print(df['source'].value_counts())
print(f"\n   By label:")
print(df['label'].value_counts())
print(f"\n   Custom recordings breakdown:")
custom_df = df[df['source'] == 'custom']
if len(custom_df) > 0:
    print(f"   - Total custom: {len(custom_df)}")
    print(f"   - Stutter: {(custom_df['label'] == 1).sum()}")
    print(f"   - Non-Stutter: {(custom_df['label'] == 0).sum()}")
else:
    print("   - No custom recordings in dataset")

# Save for inspection
df.to_csv("training_data_with_custom.csv", index=False)
print(f"\n Saved to: training_data_with_custom.csv")

# CRITICAL: Check for class imbalance
stutter_ratio = df['label'].sum() / len(df)
print(f"\n Class Balance Check:")
print(f"   Stutter ratio: {stutter_ratio:.2%}")
if stutter_ratio < 0.1 or stutter_ratio > 0.9:
    print(f"    WARNING: Highly imbalanced dataset!")
    print(f"   → Consider collecting more samples of minority class")

# STEP 5: Train-Test Split
print("\n[5/8] Splitting into train/test sets...")
X = df.drop(["label", "source"], axis=1).values
y = df["label"].values

# Stratified split to maintain class distribution
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f" Split complete:")
print(f"   Training: {len(X_train)} samples")
print(f"   Testing: {len(X_test)} samples")
print(f"   Training labels: {np.bincount(y_train)}")
print(f"   Testing labels: {np.bincount(y_test)}")
# STEP 6: Feature Scaling (CRITICAL: Use RobustScaler)
print("\n[6/8] Scaling features...")
scaler = RobustScaler()  # Better for outliers than StandardScaler
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(" Features scaled using RobustScaler")

# STEP 7: Train models with grid search
print("\n[7/8] Training models...")

# KNN Model
print("\n   Training KNN...")
knn_params = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}
knn = KNeighborsClassifier()
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='f1', n_jobs=-1)
knn_grid.fit(X_train_scaled, y_train)
best_knn = knn_grid.best_estimator_

y_pred_knn = best_knn.predict(X_test_scaled)
knn_f1 = f1_score(y_test, y_pred_knn)
print(f"  KNN - F1 Score: {knn_f1:.4f}, Best params: {knn_grid.best_params_}")

# Random Forest Model
print("\n   Training Random Forest...")
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced', None]  # Handle imbalance
}
rf = RandomForestClassifier(random_state=42)
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1', n_jobs=-1)
rf_grid.fit(X_train_scaled, y_train)
best_rf = rf_grid.best_estimator_

y_pred_rf = best_rf.predict(X_test_scaled)
rf_f1 = f1_score(y_test, y_pred_rf)
print(f" Random Forest - F1 Score: {rf_f1:.4f}, Best params: {rf_grid.best_params_}")

# Select best model
if rf_f1 > knn_f1:
    best_model = best_rf
    y_pred = y_pred_rf
    model_name = "Random Forest"
else:
    best_model = best_knn
    y_pred = y_pred_knn
    model_name = "KNN"

print(f"\n Best model: {model_name}")

# STEP 8: Evaluate
print("\n[8/8] Model Evaluation...")
print(f"\n{'='*60}")
print(f" FINAL RESULTS")
print(f"{'='*60}")
print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
print(f"\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Non-Stutter', 'Stutter']))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Non-Stutter', 'Stutter'],
            yticklabels=['Non-Stutter', 'Stutter'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title(f"Confusion Matrix - {model_name}")
plt.show()

# STEP 10: Save models
print(f"\n{'='*60}")
print("Saving models...")
joblib.dump(best_model, "stutter_model_final.pkl")
joblib.dump(scaler, "scaler_final.pkl")
joblib.dump(X_train.shape[1], "feature_count.pkl")

print(" Saved:")
print("   - stutter_model_final.pkl")
print("   - scaler_final.pkl")
print("   - feature_count.pkl")
print(f"{'='*60}\n")

print("Training complete!")